{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb = open(\"clickbait_data\",'r')\n",
    "train_data_cb = train_cb.readlines()\n",
    "replaced_blank_lines_cb = list(filter(lambda x : x != '\\n', train_data_cb))\n",
    "replaced_new_lines_cb = []\n",
    "for data in replaced_blank_lines_cb:\n",
    "    replaced_new_lines_cb.append([data.replace(\"\\n\",\"\"), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ncb = open(\"non_clickbait_data\",'r')\n",
    "train_data_ncb = train_ncb.readlines()\n",
    "replaced_blank_lines_ncb = list(filter(lambda x : x != '\\n', train_data_ncb))\n",
    "replaced_new_lines_ncb = []\n",
    "for data in replaced_blank_lines_ncb:\n",
    "    replaced_new_lines_ncb.append([data.replace(\"\\n\",\"\"), 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "crawled = replaced_new_lines_cb + replaced_new_lines_ncb\n",
    "shuffle(crawled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_data = pd.DataFrame(crawled, columns=[\"Title\", \"Clickbait\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I Hate Wearing Formals &amp; Still Cannot Understa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bhojpuri Movie Titles That Perfectly Capture T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Priyanka Chopra To Produce Comedy Series On Ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mysterious, Intimidating And Ruthless: We Fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These Illustrations On Adulthood From A Raccoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This Pune Woman’s Post About Juggling Motherho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>U.S. Defense Secretary Seeks to Reassure Europ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15 Posters That Prove Stoners Have The Most Ep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This ‘Criminal’ Shared A Photo Of Biryani With...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BookMyBai Reveals Why It Won’t Offer Its Servi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25 Indian Things You Can Eat As Much As You Wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Here’s A Guide To Keeping It Together &amp; Acting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Feds, the States and the Controlled Substa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SRK’s Raees Is A Shipwreck Of A Film &amp; He Has ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21 Lesser Known Tourist Spots To Discover In I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HBO’s Latest Superhero Drama Has A Bollywood A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Greece Orders Migrants to Declare Their Final ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Novartis to Begin Selling Copy of Amgen’s Neup...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>She Married The Love Of Her Life On Her Deathb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>China Markets: What’s Next in the Year of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Quebec Battled Similar Outbreak of Legionnaire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Valuables Of Over Rs 1 Lakh Stolen From Singer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15 Different Fart Reactions Perfectly Captured...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>‘Ma Rainey’s Black Bottom’ Review: Really the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>This Braveheart Climbed Mount Everest With One...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Odebrecht to Cooperate With Prosecutors in Cor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>U.K. Banks Healthy Enough to Weather Any EU Ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Milind Soman Wanted Company For A 7-Hour-Long ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15 Heart-Wrenching Stories From The 1984 Sikh ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Former FIFA Vice President Jeffrey Webb Agrees...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31970</th>\n",
       "      <td>U.S. Business Group Urges China to Ease Data R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31971</th>\n",
       "      <td>Beyonce’s Powerful Jubilant Return At VMA. Rih...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31972</th>\n",
       "      <td>Fed’s Policy Rate Fell Below New Target Range ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31973</th>\n",
       "      <td>CMO Today: Yahoo Formalizes Sale Process With ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31974</th>\n",
       "      <td>This Guy Prepared For The IIT Exams From Kota ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31975</th>\n",
       "      <td>With Surge On Ola/Uber You Pay A Lot. Without ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31976</th>\n",
       "      <td>Does recent market turmoil suggest a new banki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31977</th>\n",
       "      <td>New York City Students Prepare for Visit With ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31978</th>\n",
       "      <td>New Greek Finance Minister Thrown Into the Hot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31979</th>\n",
       "      <td>Tourists Reveal Which Countries They Would Nev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31980</th>\n",
       "      <td>Germany’s Anti-Immigrant Party Posts Big Gains...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31981</th>\n",
       "      <td>You Owe Your Taste Buds These 25 Food Adventur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31982</th>\n",
       "      <td>Buffett Doubles Down With Australian Insurance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31983</th>\n",
       "      <td>A Brooding Hit-Man Wanting A Better Life, Jack...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31984</th>\n",
       "      <td>13 Poignant Quotes That Aptly Capture The Mesm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31985</th>\n",
       "      <td>Meryl Streep Gives It Back To Donald Trump On ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31986</th>\n",
       "      <td>Here Are 10 Things You Shouldn’t Do In North K...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31987</th>\n",
       "      <td>Sonam Kapoor Calls Out Journalist For Pitting ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31988</th>\n",
       "      <td>Indra Nooyi &amp; Vikas Khanna Just Had Breakfast ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31989</th>\n",
       "      <td>Yahoo Cuts Digital Magazines, Closes Burbank, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31990</th>\n",
       "      <td>Officials Say Oregon Shooting Suspect Was Arme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31991</th>\n",
       "      <td>‘Live &amp; Love Today, Leave Nothing For Tomorrow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31992</th>\n",
       "      <td>10 Instagram Accounts On Mental Health To Foll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31993</th>\n",
       "      <td>Shyam Kumar’s Journey From Peon To Millionaire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>Varun Dhawan’s ‘October’ Is Not Your Typical L...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>Thailand’s Trade Surplus Hits Record High in F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>When It Comes to Tech Services, ‘Cloud’ Can Be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>10 Incredible Things You Can Do If You Have 6....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>Now Reality TV Star Kim Kardashian Receives Fl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>Three Firefighters Killed in Washington State ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  Clickbait\n",
       "0      I Hate Wearing Formals & Still Cannot Understa...          1\n",
       "1      Bhojpuri Movie Titles That Perfectly Capture T...          1\n",
       "2      Priyanka Chopra To Produce Comedy Series On Ma...          1\n",
       "3      Mysterious, Intimidating And Ruthless: We Fina...          1\n",
       "4      These Illustrations On Adulthood From A Raccoo...          1\n",
       "5      This Pune Woman’s Post About Juggling Motherho...          1\n",
       "6      U.S. Defense Secretary Seeks to Reassure Europ...          0\n",
       "7      15 Posters That Prove Stoners Have The Most Ep...          1\n",
       "8      This ‘Criminal’ Shared A Photo Of Biryani With...          1\n",
       "9      BookMyBai Reveals Why It Won’t Offer Its Servi...          1\n",
       "10     25 Indian Things You Can Eat As Much As You Wa...          1\n",
       "11     Here’s A Guide To Keeping It Together & Acting...          1\n",
       "12     The Feds, the States and the Controlled Substa...          0\n",
       "13     SRK’s Raees Is A Shipwreck Of A Film & He Has ...          1\n",
       "14     21 Lesser Known Tourist Spots To Discover In I...          1\n",
       "15     HBO’s Latest Superhero Drama Has A Bollywood A...          1\n",
       "16     Greece Orders Migrants to Declare Their Final ...          0\n",
       "17     Novartis to Begin Selling Copy of Amgen’s Neup...          0\n",
       "18     She Married The Love Of Her Life On Her Deathb...          1\n",
       "19     China Markets: What’s Next in the Year of the ...          0\n",
       "20     Quebec Battled Similar Outbreak of Legionnaire...          0\n",
       "21     Valuables Of Over Rs 1 Lakh Stolen From Singer...          1\n",
       "22     15 Different Fart Reactions Perfectly Captured...          1\n",
       "23     ‘Ma Rainey’s Black Bottom’ Review: Really the ...          0\n",
       "24     This Braveheart Climbed Mount Everest With One...          1\n",
       "25     Odebrecht to Cooperate With Prosecutors in Cor...          0\n",
       "26     U.K. Banks Healthy Enough to Weather Any EU Ex...          0\n",
       "27     Milind Soman Wanted Company For A 7-Hour-Long ...          1\n",
       "28     15 Heart-Wrenching Stories From The 1984 Sikh ...          1\n",
       "29     Former FIFA Vice President Jeffrey Webb Agrees...          0\n",
       "...                                                  ...        ...\n",
       "31970  U.S. Business Group Urges China to Ease Data R...          0\n",
       "31971  Beyonce’s Powerful Jubilant Return At VMA. Rih...          1\n",
       "31972  Fed’s Policy Rate Fell Below New Target Range ...          0\n",
       "31973  CMO Today: Yahoo Formalizes Sale Process With ...          0\n",
       "31974  This Guy Prepared For The IIT Exams From Kota ...          1\n",
       "31975  With Surge On Ola/Uber You Pay A Lot. Without ...          1\n",
       "31976  Does recent market turmoil suggest a new banki...          0\n",
       "31977  New York City Students Prepare for Visit With ...          0\n",
       "31978  New Greek Finance Minister Thrown Into the Hot...          0\n",
       "31979  Tourists Reveal Which Countries They Would Nev...          1\n",
       "31980  Germany’s Anti-Immigrant Party Posts Big Gains...          0\n",
       "31981  You Owe Your Taste Buds These 25 Food Adventur...          1\n",
       "31982  Buffett Doubles Down With Australian Insurance...          0\n",
       "31983  A Brooding Hit-Man Wanting A Better Life, Jack...          1\n",
       "31984  13 Poignant Quotes That Aptly Capture The Mesm...          1\n",
       "31985  Meryl Streep Gives It Back To Donald Trump On ...          1\n",
       "31986  Here Are 10 Things You Shouldn’t Do In North K...          1\n",
       "31987  Sonam Kapoor Calls Out Journalist For Pitting ...          1\n",
       "31988  Indra Nooyi & Vikas Khanna Just Had Breakfast ...          1\n",
       "31989  Yahoo Cuts Digital Magazines, Closes Burbank, ...          0\n",
       "31990  Officials Say Oregon Shooting Suspect Was Arme...          0\n",
       "31991  ‘Live & Love Today, Leave Nothing For Tomorrow...          1\n",
       "31992  10 Instagram Accounts On Mental Health To Foll...          1\n",
       "31993  Shyam Kumar’s Journey From Peon To Millionaire...          1\n",
       "31994  Varun Dhawan’s ‘October’ Is Not Your Typical L...          1\n",
       "31995  Thailand’s Trade Surplus Hits Record High in F...          0\n",
       "31996  When It Comes to Tech Services, ‘Cloud’ Can Be...          0\n",
       "31997  10 Incredible Things You Can Do If You Have 6....          1\n",
       "31998  Now Reality TV Star Kim Kardashian Receives Fl...          1\n",
       "31999  Three Firefighters Killed in Washington State ...          0\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawled_data = pd.read_csv(\"dataset/original/dataframe/original_train.csv\")\n",
    "# crawled_data.iloc[31984][\"Title\"]\n",
    "crawled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title        Bhojpuri Movie Titles That Perfectly Capture T...\n",
      "Clickbait                                                    1\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Preprocess shortened words like \"you ' re\" to \"you're\"\n",
    "\n",
    "for index, value in crawled_data.iterrows():\n",
    "        crawled_data.at[index, \"Title\"] = \"damn\"\n",
    "        print(crawled_data.iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many clickbait and non clickbait titles start with a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 3832\n",
      "Non-Clickbait: 8\n"
     ]
    }
   ],
   "source": [
    "count_start_digit_cb = 0\n",
    "count_start_digit_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0].isdigit():\n",
    "            count_start_digit_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0].isdigit():\n",
    "            count_start_digit_ncb += 1\n",
    "print(\"Clickbait: \"+str(count_start_digit_cb))            \n",
    "print(\"Non-Clickbait: \"+str(count_start_digit_ncb))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that mostly clickbait titles start with a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many clickbait and non clickbit titles has \"TOP + DIGIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 0\n",
      "Non-Clickbait: 0\n"
     ]
    }
   ],
   "source": [
    "count_start_top_digit_cb = 0\n",
    "count_start_top_digit_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] is \"Top\" and sp[1].isdigit():\n",
    "            count_start_top_digit_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] is \"Top\" and sp[1].isdigit():\n",
    "            count_start_top_digit_ncb += 1\n",
    "print(\"Clickbait: \"+str(count_start_top_digit_cb))            \n",
    "print(\"Non-Clickbait: \"+str(count_start_top_digit_cb))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that this data set does not contain phrases like \"Top 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the most common first word in clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_first_word_clickbait = {}\n",
    "most_common_first_word_non_clickbait = {}\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] in most_common_first_word_clickbait:\n",
    "            most_common_first_word_clickbait[sp[0]] += 1\n",
    "        else:\n",
    "            most_common_first_word_clickbait[sp[0]] = 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] in most_common_first_word_non_clickbait:\n",
    "            most_common_first_word_non_clickbait[sp[0]] += 1\n",
    "        else:\n",
    "            most_common_first_word_non_clickbait[sp[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_first_word_clickbait_df = pd.DataFrame()\n",
    "most_common_first_word_clickbait_df['Word'] = most_common_first_word_clickbait.keys()\n",
    "most_common_first_word_clickbait_df['Count'] = most_common_first_word_clickbait.values()\n",
    "\n",
    "most_common_first_word_non_clickbait_df = pd.DataFrame()\n",
    "most_common_first_word_non_clickbait_df['Word'] = most_common_first_word_non_clickbait.keys()\n",
    "most_common_first_word_non_clickbait_df['Count'] = most_common_first_word_non_clickbait.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This</td>\n",
       "      <td>1388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "5    This   1388\n",
       "4   These    726\n",
       "6      15    643\n",
       "28    The    574\n",
       "21     10    506"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_first_word_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>New</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Today’s</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Count\n",
       "0       U.S.    396\n",
       "131      New    198\n",
       "67   Today’s    170\n",
       "1        The    169\n",
       "4      China    154"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_first_word_non_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that most common first word in a clickbait title is \"This, Which\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check most common words in clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_clickbait = {}\n",
    "most_common_words_non_clickbait = {}\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        for word in sp:\n",
    "            if word in most_common_words_clickbait:\n",
    "                most_common_words_clickbait[word] += 1\n",
    "            else:\n",
    "                most_common_words_clickbait[word] = 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        for word in sp:\n",
    "            if word in most_common_words_non_clickbait:\n",
    "                most_common_words_non_clickbait[word] += 1\n",
    "            else:\n",
    "                most_common_words_non_clickbait[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_clickbait_df = pd.DataFrame()\n",
    "most_common_words_clickbait_df['Word'] = most_common_words_clickbait.keys()\n",
    "most_common_words_clickbait_df['Count'] = most_common_words_clickbait.values()\n",
    "\n",
    "most_common_words_non_clickbait_df = pd.DataFrame()\n",
    "most_common_words_non_clickbait_df['Word'] = most_common_first_word_non_clickbait.keys()\n",
    "most_common_words_non_clickbait_df['Count'] = most_common_first_word_non_clickbait.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The</td>\n",
       "      <td>7591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>To</td>\n",
       "      <td>5291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A</td>\n",
       "      <td>4741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Of</td>\n",
       "      <td>4186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>In</td>\n",
       "      <td>3724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "22   The   7591\n",
       "30    To   5291\n",
       "14     A   4741\n",
       "24    Of   4186\n",
       "103   In   3724"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>New</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Today’s</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Count\n",
       "0       U.S.    396\n",
       "131      New    198\n",
       "67   Today’s    170\n",
       "1        The    169\n",
       "4      China    154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words_non_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles contain functional words than non clickbait titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check average length of clickbait and non clickbait titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cb = []\n",
    "avg_ncb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        avg_cb.append(len(sp))\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        avg_ncb.append(len(sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Clickbait Title Lenght:  14.6718125\n",
      "Average Non-Clickbait Title Lenght:  8.96275\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Clickbait Title Lenght:  \"+str(np.array(avg_cb).mean()))\n",
    "print(\"Average Non-Clickbait Title Lenght:  \"+str(np.array(avg_ncb).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles have longer length that non-clickbait titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 62\n",
      "Nonclickbait: 3\n"
     ]
    }
   ],
   "source": [
    "the_number_cb = 0\n",
    "the_number_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        for i in range(len(sp)-1):\n",
    "            if sp[i] == \"The\" and sp[i+1].isdigit():\n",
    "                the_number_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        for i in range(len(sp)-1):\n",
    "            if sp[i] == \"The\" and sp[i+1].isdigit():\n",
    "                the_number_ncb += 1\n",
    "\n",
    "print(\"Clickbait: \"+str(the_number_cb))\n",
    "print(\"Nonclickbait: \"+str(the_number_ncb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles the bigram \"The NUMBER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stop words in clickbaits Vs NonClickbaits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopwordSeq = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    title_sw_len = 0\n",
    "    for word in title_list:\n",
    "        if word.lower() in stopWords:\n",
    "#             print(word)\n",
    "            title_sw_len += 1\n",
    "    if title_sw_len >= 3:\n",
    "        stopwordSeq.append(1)\n",
    "    else:\n",
    "        stopwordSeq.append(0)\n",
    "    \n",
    "stopwordSeq\n",
    "# crawled_data[\"stopwords\"] = stopwordSeq\n",
    "\n",
    "with open('greaterThanThreeSW.pkl', 'wb') as f:\n",
    "    pickle.dump(stopwordSeq, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  16000\n",
      "Number of clickbaits with zero stopwords->  744\n",
      "Number of non clickbaits->  16000\n",
      "Number of non clickbaits with zero stopwords->  11176\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1]))\n",
    "\n",
    "print(\"Number of clickbaits with zero stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1 and value[2] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits with zero stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0 and value[2] == 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average stopwords in clickbaits->  0.9535\n",
      "Average stopwords in Non-clickbaits->  0.3015\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "avg_sw_in_cb = mean([value[2] for index, value in crawled_data.iterrows() if value[1] == 1])\n",
    "print(\"Average stopwords in clickbaits-> \", avg_sw_in_cb)\n",
    "avg_sw_in_ncb = mean([value[2] for index, value in crawled_data.iterrows() if value[1] == 0])\n",
    "print(\"Average stopwords in Non-clickbaits-> \", avg_sw_in_ncb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 12,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 10,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 15,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 12,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 13,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 10,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 10,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 10,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 10,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 11,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 10,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 10,\n",
       " 3,\n",
       " 12,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 11,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 14,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 11,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 11,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 10,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 10,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 10,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 13,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 14,\n",
       " 2,\n",
       " 10,\n",
       " 1,\n",
       " 10,\n",
       " 11,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 10,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 13,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of stopwords in each title\n",
    "stopwordSeq1 = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    title_sw_len = 0\n",
    "    for word in title_list:\n",
    "        if word.lower() in stopWords:\n",
    "#             print(word)\n",
    "            title_sw_len += 1\n",
    "#     if title_sw_len >= 3:\n",
    "    stopwordSeq1.append(title_sw_len)\n",
    "#     else:\n",
    "#         stopwordSeq.append(0)\n",
    "    \n",
    "crawled_data[\"stopwords\"] = stopwordSeq1\n",
    "stopwordSeq1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  16000\n",
      "Number of clickbaits with atleast 3 stopwords->  15256\n",
      "Number of non clickbaits->  16000\n",
      "Number of non clickbaits with atleast 3 stopwords->  4824\n"
     ]
    }
   ],
   "source": [
    "# Adding a Threshold\n",
    "\n",
    "print(\"Number of clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1]))\n",
    "\n",
    "print(\"Number of clickbaits with atleast 3 stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1 and value[2] >= 3]))\n",
    "\n",
    "print(\"Number of non clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits with atleast 3 stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0 and value[2] >= 3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 Common Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cb_bigrams = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] == 1:\n",
    "        title_list = word_tokenize(value[0])\n",
    "        cb_bigrams.extend(list(nltk.bigrams(title_list)))\n",
    "        \n",
    "fdist = nltk.FreqDist(cb_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['’ s', '’ t', 'Here ’', '’ ll', 'You ’', 'It ’', '’ re', 'Of The', 'In The', 'Make You', 'The World', 'That ’', '& It', 'If You', 'You Can', 'Is The', 'Can ’', ', This', 'Here Are', 'That Are', 'Will Make', 'In A', 'This Is', '’ ve', 'The Most']\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "common_bigrams = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:25]\n",
    "# print(common_bigrams)\n",
    "common_bigrams = [' '.join(ct[0]) for ct in common_bigrams]\n",
    "print(common_bigrams)\n",
    "with open('commonBigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(common_bigrams, f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here ’ s', 'It ’ s', 'That ’ ll', 'Can ’ t', '& It ’', 'You ’ re', 'You ’ ll', 'Will Make You', '’ s A', '’ s Why', '’ s The', 'Don ’ t', '’ s ‘', '’ ll Make', 'India ’ s', 'We ’ re', '’ s What', '’ s How', 'You Need To', '. Here ’', ', Here ’', 'll Make You', 'This Is What', 'Didn ’ t', 'Won ’ t', ', This Is', 'They ’ re', 'He ’ s', 'There ’ s', 'If You ’', 'Around The World', 'You ’ ve', 'Game Of Thrones', '& We ’', 'In The World', 'We Can ’', '’ t Get', 'World ’ s', ', Here Are', 'You Want To', 'The World ’', 'Doesn ’ t', '’ s New', '’ t Know', ', You ’', '? Here ’', 'Of The Most', 'Who ’ s', 'That Will Make', 'All Of Us']\n"
     ]
    }
   ],
   "source": [
    "cb_trigrams = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] == 1:\n",
    "        title_list = word_tokenize(value[0])\n",
    "        c = 0\n",
    "        while c < len(title_list) - 2:\n",
    "            cb_trigrams.append((title_list[c], title_list[c+1], title_list[c+2]))\n",
    "            c += 1\n",
    "#         cb_trigrams.extend(list(nltk.trigrams(title_list)))\n",
    "        \n",
    "fdisttri = nltk.FreqDist(cb_trigrams)\n",
    "common_trigrams = sorted(fdisttri.items(), key=itemgetter(1), reverse=True)[:50]\n",
    "common_trigrams = [' '.join(ct[0]) for ct in common_trigrams]\n",
    "print(common_trigrams)\n",
    "with open('commonTrigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(common_trigrams, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 stopwords with which clickbaits start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbaits that start with stopword->  7148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'Which', 'What', 'The', 'How', 'We', 'Can', 'Here', 'Are', 'These']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordStart_cb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    if cb == 1:\n",
    "        if title_list[0].lower() in stopWords:\n",
    "            stopwordStart_cb.append(title_list[0])\n",
    "    \n",
    "print(\"clickbaits that start with stopword-> \", len(stopwordStart_cb))\n",
    "stopwordStart_cb\n",
    "fdist = nltk.FreqDist(stopwordStart_cb)\n",
    "stopwordStart_cb_freq = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "stopwordStart_cb_freq = [x[0] for x in stopwordStart_cb_freq]\n",
    "stopwordStart_cb_freq\n",
    "with open('startWithStopWords.pkl', 'wb') as f:\n",
    "    pickle.dump(stopwordStart_cb_freq, f)\n",
    "stopwordStart_cb_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-clickbaits that start with stopword->  1261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'In', 'The', 'At', 'For', 'As', 'With', 'After', 'No', 'On']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordStart_ncb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    if cb == 0:\n",
    "        if title_list[0].lower() in stopWords:\n",
    "            stopwordStart_ncb.append(title_list[0])\n",
    "    \n",
    "print(\"Non-clickbaits that start with stopword-> \", len(stopwordStart_ncb))\n",
    "\n",
    "fdist = nltk.FreqDist(stopwordStart_ncb)\n",
    "stopwordStart_ncb_freq = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "stopwordStart_ncb_freq = [x[0] for x in stopwordStart_ncb_freq]\n",
    "stopwordStart_ncb_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortened Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_wrds = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = title.split()\n",
    "    if cb == 1:\n",
    "        for title in title_list:\n",
    "            t = title\n",
    "            apos_present = False\n",
    "            for ch in title:\n",
    "                if ch == \"'\":\n",
    "                    apos_present = True\n",
    "            if apos_present:\n",
    "                shortened_wrds.append(title)\n",
    "shortened_wrds\n",
    "fsdist = nltk.FreqDist(shortened_wrds)\n",
    "shortened_wrds_freq = sorted(fsdist.items(), key=itemgetter(1), reverse=True)[:29]\n",
    "shortened_wrds_freq = [x[0] for x in shortened_wrds_freq]\n",
    "shortened_wrds_freq.extend([\"That's\",\"Shouldn't\",\"Everyone's\",\"Haven't\"])\n",
    "shortened_wrds_freq\n",
    "with open('shortenedWords.pkl', 'wb') as f:\n",
    "    pickle.dump(shortened_wrds_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
