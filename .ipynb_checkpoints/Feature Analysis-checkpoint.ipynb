{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb = open(\"clickbait_data\",'r')\n",
    "train_data_cb = train_cb.readlines()\n",
    "replaced_blank_lines_cb = list(filter(lambda x : x != '\\n', train_data_cb))\n",
    "replaced_new_lines_cb = []\n",
    "for data in replaced_blank_lines_cb:\n",
    "    replaced_new_lines_cb.append([data.replace(\"\\n\",\"\"), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ncb = open(\"non_clickbait_data\",'r')\n",
    "train_data_ncb = train_ncb.readlines()\n",
    "replaced_blank_lines_ncb = list(filter(lambda x : x != '\\n', train_data_ncb))\n",
    "replaced_new_lines_ncb = []\n",
    "for data in replaced_blank_lines_ncb:\n",
    "    replaced_new_lines_ncb.append([data.replace(\"\\n\",\"\"), 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "crawled = replaced_new_lines_cb + replaced_new_lines_ncb\n",
    "shuffle(crawled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_data = pd.DataFrame(crawled, columns=[\"Title\", \"Clickbait\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many clickbait and non clickbait titles start with a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 5940\n",
      "Non-Clickbait: 306\n"
     ]
    }
   ],
   "source": [
    "count_start_digit_cb = 0\n",
    "count_start_digit_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0].isdigit():\n",
    "            count_start_digit_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0].isdigit():\n",
    "            count_start_digit_ncb += 1\n",
    "print(\"Clickbait: \"+str(count_start_digit_cb))            \n",
    "print(\"Non-Clickbait: \"+str(count_start_digit_ncb))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that mostly clickbait titles start with a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many clickbait and non clickbit titles has \"TOP + DIGIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 0\n",
      "Non-Clickbait: 0\n"
     ]
    }
   ],
   "source": [
    "count_start_top_digit_cb = 0\n",
    "count_start_top_digit_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] is \"Top\" and sp[1].isdigit():\n",
    "            count_start_top_digit_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] is \"Top\" and sp[1].isdigit():\n",
    "            count_start_top_digit_ncb += 1\n",
    "print(\"Clickbait: \"+str(count_start_top_digit_cb))            \n",
    "print(\"Non-Clickbait: \"+str(count_start_top_digit_cb))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that this data set does not contain phrases like \"Top 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the most common first word in clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_first_word_clickbait = {}\n",
    "most_common_first_word_non_clickbait = {}\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] in most_common_first_word_clickbait:\n",
    "            most_common_first_word_clickbait[sp[0]] += 1\n",
    "        else:\n",
    "            most_common_first_word_clickbait[sp[0]] = 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] in most_common_first_word_non_clickbait:\n",
    "            most_common_first_word_non_clickbait[sp[0]] += 1\n",
    "        else:\n",
    "            most_common_first_word_non_clickbait[sp[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_first_word_clickbait_df = pd.DataFrame()\n",
    "most_common_first_word_clickbait_df['Word'] = most_common_first_word_clickbait.keys()\n",
    "most_common_first_word_clickbait_df['Count'] = most_common_first_word_clickbait.values()\n",
    "\n",
    "most_common_first_word_non_clickbait_df = pd.DataFrame()\n",
    "most_common_first_word_non_clickbait_df['Word'] = most_common_first_word_non_clickbait.keys()\n",
    "most_common_first_word_non_clickbait_df['Count'] = most_common_first_word_non_clickbait.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Which</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>21</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "5    This   1071\n",
       "15  Which    754\n",
       "11     17    646\n",
       "43     21    618\n",
       "23    The    607"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_first_word_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>US</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>New</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>A</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "59     US    299\n",
       "277   New    235\n",
       "31   U.S.    212\n",
       "88      A    205\n",
       "0      In    176"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_first_word_non_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that most common first word in a clickbait title is \"This, Which\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check most common words in clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_clickbait = {}\n",
    "most_common_words_non_clickbait = {}\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        for word in sp:\n",
    "            if word in most_common_words_clickbait:\n",
    "                most_common_words_clickbait[word] += 1\n",
    "            else:\n",
    "                most_common_words_clickbait[word] = 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        for word in sp:\n",
    "            if word in most_common_words_non_clickbait:\n",
    "                most_common_words_non_clickbait[word] += 1\n",
    "            else:\n",
    "                most_common_words_non_clickbait[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_clickbait_df = pd.DataFrame()\n",
    "most_common_words_clickbait_df['Word'] = most_common_words_clickbait.keys()\n",
    "most_common_words_clickbait_df['Count'] = most_common_words_clickbait.values()\n",
    "\n",
    "most_common_words_non_clickbait_df = pd.DataFrame()\n",
    "most_common_words_non_clickbait_df['Word'] = most_common_first_word_non_clickbait.keys()\n",
    "most_common_words_non_clickbait_df['Count'] = most_common_first_word_non_clickbait.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You</td>\n",
       "      <td>4804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>To</td>\n",
       "      <td>3231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>A</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Your</td>\n",
       "      <td>2536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "8    You   4804\n",
       "18   The   4711\n",
       "41    To   3231\n",
       "72     A   2600\n",
       "51  Your   2536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>US</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>New</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>A</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "59     US    299\n",
       "277   New    235\n",
       "31   U.S.    212\n",
       "88      A    205\n",
       "0      In    176"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words_non_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles contain functional words than non clickbait titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check average length of clickbait and non clickbait titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cb = []\n",
    "avg_ncb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        avg_cb.append(len(sp))\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        avg_ncb.append(len(sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Clickbait Title Lenght:  9.942683917744858\n",
      "Average Non-Clickbait Title Lenght:  8.19498781326167\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Clickbait Title Lenght:  \"+str(np.array(avg_cb).mean()))\n",
    "print(\"Average Non-Clickbait Title Lenght:  \"+str(np.array(avg_ncb).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles have longer length that non-clickbait titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 217\n",
      "Nonclickbait: 1\n"
     ]
    }
   ],
   "source": [
    "the_number_cb = 0\n",
    "the_number_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        for i in range(len(sp)-1):\n",
    "            if sp[i] == \"The\" and sp[i+1].isdigit():\n",
    "                the_number_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        for i in range(len(sp)-1):\n",
    "            if sp[i] == \"The\" and sp[i+1].isdigit():\n",
    "                the_number_ncb += 1\n",
    "\n",
    "print(\"Clickbait: \"+str(the_number_cb))\n",
    "print(\"Nonclickbait: \"+str(the_number_ncb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles the bigram \"The NUMBER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stop words in clickbaits Vs NonClickbaits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopwordSeq = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    title_sw_len = 0\n",
    "    for word in title_list:\n",
    "        if word.lower() in stopWords:\n",
    "#             print(word)\n",
    "            title_sw_len += 1\n",
    "    if title_sw_len >= 3:\n",
    "        stopwordSeq.append(1)\n",
    "    else:\n",
    "        stopwordSeq.append(0)\n",
    "    \n",
    "stopwordSeq\n",
    "# crawled_data[\"stopwords\"] = stopwordSeq\n",
    "\n",
    "with open('greaterThanThreeSW.pkl', 'wb') as f:\n",
    "    pickle.dump(common_bigrams, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  15999\n",
      "Number of clickbaits with zero stopwords->  297\n",
      "Number of non clickbaits->  16001\n",
      "Number of non clickbaits with zero stopwords->  2209\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1]))\n",
    "\n",
    "print(\"Number of clickbaits with zero stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1 and value[2] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits with zero stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0 and value[2] == 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average stopwords in clickbaits->  3.911744484030252\n",
      "Average stopwords in Non-clickbaits->  1.725829635647772\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "avg_sw_in_cb = mean([value[2] for index, value in crawled_data.iterrows() if value[1] == 1])\n",
    "print(\"Average stopwords in clickbaits-> \", avg_sw_in_cb)\n",
    "avg_sw_in_ncb = mean([value[2] for index, value in crawled_data.iterrows() if value[1] == 0])\n",
    "print(\"Average stopwords in Non-clickbaits-> \", avg_sw_in_ncb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  15999\n",
      "Number of clickbaits with atleast 3 stopwords->  12451\n",
      "Number of non clickbaits->  16001\n",
      "Number of non clickbaits with atleast 3 stopwords->  3642\n"
     ]
    }
   ],
   "source": [
    "# Adding a Threshold\n",
    "\n",
    "print(\"Number of clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1]))\n",
    "\n",
    "print(\"Number of clickbaits with atleast 3 stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1 and value[2] >= 3]))\n",
    "\n",
    "print(\"Number of non clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits with atleast 3 stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0 and value[2] >= 3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 Common Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cb_bigrams = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] == 1:\n",
    "        title_list = word_tokenize(value[0])\n",
    "        cb_bigrams.extend(list(nltk.bigrams(title_list)))\n",
    "        \n",
    "fdist = nltk.FreqDist(cb_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are You', 'Based On', 'Do You', 'On Your', 'That Will', \"Here 's\", 'Can You', 'Make You', 'Of The', 'That Are', 'We Know', 'Will Make', 'The Most', \"You 're\", 'People Who', 'The Best', \"It 's\", 'If You', 'Need To', 'And It', 'Your Zodiac', 'In The', 'Things You', 'In 2015', 'This Is']\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "common_bigrams = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:25]\n",
    "# print(common_bigrams)\n",
    "common_bigrams = [' '.join(ct[0]) for ct in common_bigrams]\n",
    "print(common_bigrams)\n",
    "with open('commonBigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(common_bigrams, f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Based On Your', 'Will Make You', 'That Will Make', 'On Your Zodiac', 'Your Zodiac Sign', 'How Well Do', 'Well Do You', 'You Need To', \"Here 's What\", 'This Is What', 'Are You More', \"And It 's\", 'You Based On', 'Are You Based', 'We Know Your', 'Make You Laugh', '`` Harry Potter', 'Do You Remember', 'Do You Know', \"What 's The\", \"If You 're\", \"Harry Potter ''\", 'Of The Most', '`` Star Wars', 'Can We Guess', \"Here 's How\", 'Can You Guess', 'For People Who', 'Character Are You', 'For The First', 'The First Time', 'On Your Favorite', \"'' Character Are\", \"Star Wars ''\", \"Things You 'll\", 'Too Real For', 'Can You Identify', 'Do You Actually', 'Way Too Real', \"What It 's\", 'Of All Time', 'That Are Way', \"That 'll Make\", '`` Game Of', 'Game Of Thrones', \"Of Thrones ''\", 'We Need To', 'And It Was', 'From `` The', 'To Talk About']\n"
     ]
    }
   ],
   "source": [
    "cb_trigrams = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] == 1:\n",
    "        title_list = word_tokenize(value[0])\n",
    "        c = 0\n",
    "        while c < len(title_list) - 2:\n",
    "            cb_trigrams.append((title_list[c], title_list[c+1], title_list[c+2]))\n",
    "            c += 1\n",
    "#         cb_trigrams.extend(list(nltk.trigrams(title_list)))\n",
    "        \n",
    "fdisttri = nltk.FreqDist(cb_trigrams)\n",
    "common_trigrams = sorted(fdisttri.items(), key=itemgetter(1), reverse=True)[:50]\n",
    "common_trigrams = [' '.join(ct[0]) for ct in common_trigrams]\n",
    "print(common_trigrams)\n",
    "with open('commonTrigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(common_trigrams, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 stopwords with which clickbaits start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbaits that start with stopword->  7148\n"
     ]
    }
   ],
   "source": [
    "stopwordStart_cb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    if cb == 1:\n",
    "        if title_list[0].lower() in stopWords:\n",
    "            stopwordStart_cb.append(title_list[0])\n",
    "    \n",
    "print(\"clickbaits that start with stopword-> \", len(stopwordStart_cb))\n",
    "stopwordStart_cb\n",
    "fdist = nltk.FreqDist(stopwordStart_cb)\n",
    "stopwordStart_cb_freq = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "stopwordStart_cb_freq = [x[0] for x in stopwordStart_cb_freq]\n",
    "stopwordStart_cb_freq\n",
    "with open('startWithStopWords.pkl', 'wb') as f:\n",
    "    pickle.dump(stopwordStart_cb_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-clickbaits that start with stopword->  1261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'In', 'The', 'At', 'For', 'As', 'With', 'After', 'No', 'On']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordStart_ncb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    if cb == 0:\n",
    "        if title_list[0].lower() in stopWords:\n",
    "            stopwordStart_ncb.append(title_list[0])\n",
    "    \n",
    "print(\"Non-clickbaits that start with stopword-> \", len(stopwordStart_ncb))\n",
    "\n",
    "fdist = nltk.FreqDist(stopwordStart_ncb)\n",
    "stopwordStart_ncb_freq = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "stopwordStart_ncb_freq = [x[0] for x in stopwordStart_ncb_freq]\n",
    "stopwordStart_ncb_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
