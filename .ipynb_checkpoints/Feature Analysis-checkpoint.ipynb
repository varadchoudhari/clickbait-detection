{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cb = open(\"clickbait_data\",'r')\n",
    "train_data_cb = train_cb.readlines()\n",
    "replaced_blank_lines_cb = list(filter(lambda x : x != '\\n', train_data_cb))\n",
    "replaced_new_lines_cb = []\n",
    "for data in replaced_blank_lines_cb:\n",
    "    replaced_new_lines_cb.append([data.replace(\"\\n\",\"\"), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ncb = open(\"non_clickbait_data\",'r')\n",
    "train_data_ncb = train_ncb.readlines()\n",
    "replaced_blank_lines_ncb = list(filter(lambda x : x != '\\n', train_data_ncb))\n",
    "replaced_new_lines_ncb = []\n",
    "for data in replaced_blank_lines_ncb:\n",
    "    replaced_new_lines_ncb.append([data.replace(\"\\n\",\"\"), 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "crawled = replaced_new_lines_cb + replaced_new_lines_ncb\n",
    "shuffle(crawled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_data = pd.DataFrame(crawled, columns=[\"Title\", \"Clickbait\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9 Realities For People Who Are Delicate Fuckin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lindsay Lohan surrenders to police</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Employment Contracts Are Now Viewed as Rewritable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UN health expert warns Bird Flu could kill 150...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Find me all the red balloons; MIT wins DARPA c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>700 Merrill Employees Received $1 Million Bonuses</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An Exact Replica Of The Titanic Will Set Sail ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TV Land Tries to Spice Things Up With Reality ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Here's Our First Look At Season 2 Of \"Outlande...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Explosion, fire temporarily halts cereal produ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7 \"Scream Queens\" Questions That Are Impossibl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Former Syrian minister of defense defects to T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21 Little Joys Tall People Will Never Get To E...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>These Two Adorable Guinea Pigs Sharing A Blade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stop What You're Doing And Listen To This Remi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22 Times People On Facebook Issued Some Seriou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Rhoden: In Detroit and Oakland, Coaches Combat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Russian Orthodox Church Elects Outspoken Patri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Romanian parliament ratifies EU accession treaty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17 Reasons Tina Belcher Is The Best Role Model...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12 dead following police raid of Mexican night...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Your Horoscope For The Week Of November 30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Thornton Helps L.S.U. Edge Butler</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Jefferson to face forward on new nickel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What Life-Changing Product Do You Wish You'd O...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Here Are 18 Romantic AF Movies &amp; Shows To Stre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>22 Times Animals Were You Just Trying To Live ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What's Your Favorite Month Based On Your Zodia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>People Are Loving The Flower Headphones Lana D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>British computer firm Tiny axes more than 1,50...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31970</th>\n",
       "      <td>Awkward Dance for E.U. Treaty Talks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31971</th>\n",
       "      <td>J.K. Rowling Is Splitting \"Harry Potter And Th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31972</th>\n",
       "      <td>Google Street View comes indoors</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31973</th>\n",
       "      <td>Last-Second Shot Sends Villanova Past Pitt and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31974</th>\n",
       "      <td>17 Red Carpet Looks Of 2015 That Were Breathta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31975</th>\n",
       "      <td>UK MPs vote not to lower abortion limit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31976</th>\n",
       "      <td>US may not have known of Sgrena rescue operation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31977</th>\n",
       "      <td>More Customers Give Up the Cellphone Contract</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31978</th>\n",
       "      <td>21 Awesome Ways To Bring The Outdoors Into You...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31979</th>\n",
       "      <td>Spy Tale Roils Muddied Waters in Cup Dispute</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31980</th>\n",
       "      <td>Car bomb in Ciudad Juárez, Mexico kills severa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31981</th>\n",
       "      <td>Rhode Island borrows $90 million from US for j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31982</th>\n",
       "      <td>At least 22 killed in blast in central Pakistan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31983</th>\n",
       "      <td>Something's Wrong With Santa's Reindeer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31984</th>\n",
       "      <td>I Saw \"The Force Awakens\" Without Seeing Any O...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31985</th>\n",
       "      <td>Which Starbucks Holiday Beverage Are You</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31986</th>\n",
       "      <td>Americans Review Australian Currency</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31987</th>\n",
       "      <td>Two dead after two light aircraft crash in Isl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31988</th>\n",
       "      <td>British Unions, Angry Over Use of Foreign Work...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31989</th>\n",
       "      <td>Kaley Cuoco Performed \"I'm A Slave 4 U\" With A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31990</th>\n",
       "      <td>After Marathon Haul, Phelps Turns to Sprints</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31991</th>\n",
       "      <td>The Iconic Beatles Ashram In Rishikesh Is Once...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31992</th>\n",
       "      <td>Jenson Button wins Hungarian Grand Prix</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31993</th>\n",
       "      <td>Little Girls Confidently Give Women Advice On ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>27 Emotions Every Actor Should Know</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>20 Startling Boob Confessions Everyone Should See</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>What Percent Vegan Are You</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>11 Pirates Seized by French Navy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>Here's A List Of Wigs Adele Has Snatched This ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>Chinese Exports Decline, but More Slowly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  Clickbait\n",
       "0      9 Realities For People Who Are Delicate Fuckin...          1\n",
       "1                     Lindsay Lohan surrenders to police          0\n",
       "2      Employment Contracts Are Now Viewed as Rewritable          0\n",
       "3      UN health expert warns Bird Flu could kill 150...          0\n",
       "4      Find me all the red balloons; MIT wins DARPA c...          0\n",
       "5      700 Merrill Employees Received $1 Million Bonuses          0\n",
       "6      An Exact Replica Of The Titanic Will Set Sail ...          1\n",
       "7      TV Land Tries to Spice Things Up With Reality ...          0\n",
       "8      Here's Our First Look At Season 2 Of \"Outlande...          1\n",
       "9      Explosion, fire temporarily halts cereal produ...          0\n",
       "10     7 \"Scream Queens\" Questions That Are Impossibl...          1\n",
       "11     Former Syrian minister of defense defects to T...          0\n",
       "12     21 Little Joys Tall People Will Never Get To E...          1\n",
       "13     These Two Adorable Guinea Pigs Sharing A Blade...          1\n",
       "14     Stop What You're Doing And Listen To This Remi...          1\n",
       "15     22 Times People On Facebook Issued Some Seriou...          1\n",
       "16     Rhoden: In Detroit and Oakland, Coaches Combat...          0\n",
       "17     Russian Orthodox Church Elects Outspoken Patri...          0\n",
       "18      Romanian parliament ratifies EU accession treaty          0\n",
       "19     17 Reasons Tina Belcher Is The Best Role Model...          1\n",
       "20     12 dead following police raid of Mexican night...          0\n",
       "21            Your Horoscope For The Week Of November 30          1\n",
       "22                     Thornton Helps L.S.U. Edge Butler          0\n",
       "23               Jefferson to face forward on new nickel          0\n",
       "24     What Life-Changing Product Do You Wish You'd O...          1\n",
       "25     Here Are 18 Romantic AF Movies & Shows To Stre...          1\n",
       "26     22 Times Animals Were You Just Trying To Live ...          1\n",
       "27     What's Your Favorite Month Based On Your Zodia...          1\n",
       "28     People Are Loving The Flower Headphones Lana D...          1\n",
       "29     British computer firm Tiny axes more than 1,50...          0\n",
       "...                                                  ...        ...\n",
       "31970                Awkward Dance for E.U. Treaty Talks          0\n",
       "31971  J.K. Rowling Is Splitting \"Harry Potter And Th...          1\n",
       "31972                   Google Street View comes indoors          0\n",
       "31973  Last-Second Shot Sends Villanova Past Pitt and...          0\n",
       "31974  17 Red Carpet Looks Of 2015 That Were Breathta...          1\n",
       "31975            UK MPs vote not to lower abortion limit          0\n",
       "31976   US may not have known of Sgrena rescue operation          0\n",
       "31977      More Customers Give Up the Cellphone Contract          0\n",
       "31978  21 Awesome Ways To Bring The Outdoors Into You...          1\n",
       "31979       Spy Tale Roils Muddied Waters in Cup Dispute          0\n",
       "31980  Car bomb in Ciudad Juárez, Mexico kills severa...          0\n",
       "31981  Rhode Island borrows $90 million from US for j...          0\n",
       "31982    At least 22 killed in blast in central Pakistan          0\n",
       "31983            Something's Wrong With Santa's Reindeer          1\n",
       "31984  I Saw \"The Force Awakens\" Without Seeing Any O...          1\n",
       "31985           Which Starbucks Holiday Beverage Are You          1\n",
       "31986               Americans Review Australian Currency          1\n",
       "31987  Two dead after two light aircraft crash in Isl...          0\n",
       "31988  British Unions, Angry Over Use of Foreign Work...          0\n",
       "31989  Kaley Cuoco Performed \"I'm A Slave 4 U\" With A...          1\n",
       "31990       After Marathon Haul, Phelps Turns to Sprints          0\n",
       "31991  The Iconic Beatles Ashram In Rishikesh Is Once...          1\n",
       "31992            Jenson Button wins Hungarian Grand Prix          0\n",
       "31993  Little Girls Confidently Give Women Advice On ...          1\n",
       "31994                27 Emotions Every Actor Should Know          1\n",
       "31995  20 Startling Boob Confessions Everyone Should See          1\n",
       "31996                         What Percent Vegan Are You          1\n",
       "31997                   11 Pirates Seized by French Navy          0\n",
       "31998  Here's A List Of Wigs Adele Has Snatched This ...          1\n",
       "31999           Chinese Exports Decline, but More Slowly          0\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawled_data = pd.read_csv(\"DATASET.csv\")\n",
    "crawled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many clickbait and non clickbait titles start with a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 5940\n",
      "Non-Clickbait: 306\n"
     ]
    }
   ],
   "source": [
    "count_start_digit_cb = 0\n",
    "count_start_digit_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0].isdigit():\n",
    "            count_start_digit_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0].isdigit():\n",
    "            count_start_digit_ncb += 1\n",
    "print(\"Clickbait: \"+str(count_start_digit_cb))            \n",
    "print(\"Non-Clickbait: \"+str(count_start_digit_ncb))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that mostly clickbait titles start with a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many clickbait and non clickbit titles has \"TOP + DIGIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 0\n",
      "Non-Clickbait: 0\n"
     ]
    }
   ],
   "source": [
    "count_start_top_digit_cb = 0\n",
    "count_start_top_digit_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] is \"Top\" and sp[1].isdigit():\n",
    "            count_start_top_digit_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] is \"Top\" and sp[1].isdigit():\n",
    "            count_start_top_digit_ncb += 1\n",
    "print(\"Clickbait: \"+str(count_start_top_digit_cb))            \n",
    "print(\"Non-Clickbait: \"+str(count_start_top_digit_cb))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that this data set does not contain phrases like \"Top 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the most common first word in clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_first_word_clickbait = {}\n",
    "most_common_first_word_non_clickbait = {}\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] in most_common_first_word_clickbait:\n",
    "            most_common_first_word_clickbait[sp[0]] += 1\n",
    "        else:\n",
    "            most_common_first_word_clickbait[sp[0]] = 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] in most_common_first_word_non_clickbait:\n",
    "            most_common_first_word_non_clickbait[sp[0]] += 1\n",
    "        else:\n",
    "            most_common_first_word_non_clickbait[sp[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_first_word_clickbait_df = pd.DataFrame()\n",
    "most_common_first_word_clickbait_df['Word'] = most_common_first_word_clickbait.keys()\n",
    "most_common_first_word_clickbait_df['Count'] = most_common_first_word_clickbait.values()\n",
    "\n",
    "most_common_first_word_non_clickbait_df = pd.DataFrame()\n",
    "most_common_first_word_non_clickbait_df['Word'] = most_common_first_word_non_clickbait.keys()\n",
    "most_common_first_word_non_clickbait_df['Count'] = most_common_first_word_non_clickbait.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>This</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Which</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "36   This   1071\n",
       "30  Which    754\n",
       "8      17    646\n",
       "4      21    618\n",
       "17    The    607"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_first_word_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>US</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>New</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>A</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>In</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "169    US    299\n",
       "39    New    235\n",
       "21   U.S.    212\n",
       "140     A    205\n",
       "49     In    176"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_first_word_non_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that most common first word in a clickbait title is \"This, Which\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check most common words in clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_clickbait = {}\n",
    "most_common_words_non_clickbait = {}\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        for word in sp:\n",
    "            if word in most_common_words_clickbait:\n",
    "                most_common_words_clickbait[word] += 1\n",
    "            else:\n",
    "                most_common_words_clickbait[word] = 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        for word in sp:\n",
    "            if word in most_common_words_non_clickbait:\n",
    "                most_common_words_non_clickbait[word] += 1\n",
    "            else:\n",
    "                most_common_words_non_clickbait[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_clickbait_df = pd.DataFrame()\n",
    "most_common_words_clickbait_df['Word'] = most_common_words_clickbait.keys()\n",
    "most_common_words_clickbait_df['Count'] = most_common_words_clickbait.values()\n",
    "\n",
    "most_common_words_non_clickbait_df = pd.DataFrame()\n",
    "most_common_words_non_clickbait_df['Word'] = most_common_first_word_non_clickbait.keys()\n",
    "most_common_words_non_clickbait_df['Count'] = most_common_first_word_non_clickbait.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>You</td>\n",
       "      <td>4804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>To</td>\n",
       "      <td>3231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>A</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Your</td>\n",
       "      <td>2536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "96   You   4804\n",
       "13   The   4711\n",
       "38    To   3231\n",
       "53     A   2600\n",
       "59  Your   2536"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>US</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>New</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>A</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>In</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "169    US    299\n",
       "39    New    235\n",
       "21   U.S.    212\n",
       "140     A    205\n",
       "49     In    176"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words_non_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles contain functional words than non clickbait titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check average length of clickbait and non clickbait titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cb = []\n",
    "avg_ncb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        avg_cb.append(len(sp))\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        avg_ncb.append(len(sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Clickbait Title Lenght:  9.942683917744858\n",
      "Average Non-Clickbait Title Lenght:  8.19498781326167\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Clickbait Title Lenght:  \"+str(np.array(avg_cb).mean()))\n",
    "print(\"Average Non-Clickbait Title Lenght:  \"+str(np.array(avg_ncb).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles have longer length that non-clickbait titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 217\n",
      "Nonclickbait: 1\n"
     ]
    }
   ],
   "source": [
    "the_number_cb = 0\n",
    "the_number_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        for i in range(len(sp)-1):\n",
    "            if sp[i] == \"The\" and sp[i+1].isdigit():\n",
    "                the_number_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        for i in range(len(sp)-1):\n",
    "            if sp[i] == \"The\" and sp[i+1].isdigit():\n",
    "                the_number_ncb += 1\n",
    "\n",
    "print(\"Clickbait: \"+str(the_number_cb))\n",
    "print(\"Nonclickbait: \"+str(the_number_ncb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles the bigram \"The NUMBER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stop words in clickbaits Vs NonClickbaits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopwordSeq = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    title_sw_len = 0\n",
    "    for word in title_list:\n",
    "        if word.lower() in stopWords:\n",
    "#             print(word)\n",
    "            title_sw_len += 1\n",
    "    if title_sw_len >= 3:\n",
    "        stopwordSeq.append(1)\n",
    "    else:\n",
    "        stopwordSeq.append(0)\n",
    "    \n",
    "stopwordSeq\n",
    "crawled_data[\"stopwords\"] = stopwordSeq\n",
    "\n",
    "with open('greaterThanThreeSW.pkl', 'wb') as f:\n",
    "    pickle.dump(stopwordSeq, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  15999\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-2a4a8b0b1ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of clickbaits-> \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of clickbaits with zero stopwords-> \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of non clickbaits-> \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-2a4a8b0b1ca2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of clickbaits-> \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of clickbaits with zero stopwords-> \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of non clickbaits-> \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3124\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3125\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3126\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of bounds"
     ]
    }
   ],
   "source": [
    "print(\"Number of clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1]))\n",
    "\n",
    "print(\"Number of clickbaits with zero stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1 and value[2] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits with zero stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0 and value[2] == 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-abea1d9d9832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mavg_sw_in_cb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average stopwords in clickbaits-> \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_sw_in_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavg_sw_in_ncb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average stopwords in Non-clickbaits-> \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_sw_in_ncb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-143-abea1d9d9832>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mavg_sw_in_cb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average stopwords in clickbaits-> \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_sw_in_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavg_sw_in_ncb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average stopwords in Non-clickbaits-> \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_sw_in_ncb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3124\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3125\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3126\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of bounds"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "avg_sw_in_cb = mean([value[2] for index, value in crawled_data.iterrows() if value[1] == 1])\n",
    "print(\"Average stopwords in clickbaits-> \", avg_sw_in_cb)\n",
    "avg_sw_in_ncb = mean([value[2] for index, value in crawled_data.iterrows() if value[1] == 0])\n",
    "print(\"Average stopwords in Non-clickbaits-> \", avg_sw_in_ncb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  15999\n",
      "Number of clickbaits with atleast 3 stopwords->  12451\n",
      "Number of non clickbaits->  16001\n",
      "Number of non clickbaits with atleast 3 stopwords->  3642\n"
     ]
    }
   ],
   "source": [
    "# Adding a Threshold\n",
    "\n",
    "print(\"Number of clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1]))\n",
    "\n",
    "print(\"Number of clickbaits with atleast 3 stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1 and value[2] >= 3]))\n",
    "\n",
    "print(\"Number of non clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits with atleast 3 stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0 and value[2] >= 3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 Common Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cb_bigrams = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] == 1:\n",
    "        title_list = word_tokenize(value[0])\n",
    "        cb_bigrams.extend(list(nltk.bigrams(title_list)))\n",
    "        \n",
    "fdist = nltk.FreqDist(cb_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are You', 'Based On', 'Do You', 'On Your', 'That Will', \"Here 's\", 'Can You', 'Make You', 'Of The', 'That Are', 'We Know', 'Will Make', 'The Most', \"You 're\", 'People Who', 'The Best', \"It 's\", 'If You', 'Need To', 'And It', 'Your Zodiac', 'In The', 'Things You', 'In 2015', 'This Is']\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "common_bigrams = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:25]\n",
    "# print(common_bigrams)\n",
    "common_bigrams = [' '.join(ct[0]) for ct in common_bigrams]\n",
    "print(common_bigrams)\n",
    "with open('commonBigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(common_bigrams, f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Based On Your', 'Will Make You', 'That Will Make', 'On Your Zodiac', 'Your Zodiac Sign', 'How Well Do', 'Well Do You', 'You Need To', \"Here 's What\", 'This Is What', 'Are You More', \"And It 's\", 'You Based On', 'Are You Based', 'We Know Your', 'Make You Laugh', '`` Harry Potter', 'Do You Remember', 'Do You Know', \"What 's The\", \"If You 're\", \"Harry Potter ''\", 'Of The Most', '`` Star Wars', 'Can We Guess', \"Here 's How\", 'Can You Guess', 'For People Who', 'Character Are You', 'For The First', 'The First Time', 'On Your Favorite', \"'' Character Are\", \"Star Wars ''\", \"Things You 'll\", 'Too Real For', 'Can You Identify', 'Do You Actually', 'Way Too Real', \"What It 's\", 'Of All Time', 'That Are Way', \"That 'll Make\", '`` Game Of', 'Game Of Thrones', \"Of Thrones ''\", 'We Need To', 'And It Was', 'From `` The', 'To Talk About']\n"
     ]
    }
   ],
   "source": [
    "cb_trigrams = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] == 1:\n",
    "        title_list = word_tokenize(value[0])\n",
    "        c = 0\n",
    "        while c < len(title_list) - 2:\n",
    "            cb_trigrams.append((title_list[c], title_list[c+1], title_list[c+2]))\n",
    "            c += 1\n",
    "#         cb_trigrams.extend(list(nltk.trigrams(title_list)))\n",
    "        \n",
    "fdisttri = nltk.FreqDist(cb_trigrams)\n",
    "common_trigrams = sorted(fdisttri.items(), key=itemgetter(1), reverse=True)[:50]\n",
    "common_trigrams = [' '.join(ct[0]) for ct in common_trigrams]\n",
    "print(common_trigrams)\n",
    "with open('commonTrigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(common_trigrams, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 stopwords with which clickbaits start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickbaits that start with stopword->  7148\n"
     ]
    }
   ],
   "source": [
    "stopwordStart_cb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    if cb == 1:\n",
    "        if title_list[0].lower() in stopWords:\n",
    "            stopwordStart_cb.append(title_list[0])\n",
    "    \n",
    "print(\"clickbaits that start with stopword-> \", len(stopwordStart_cb))\n",
    "stopwordStart_cb\n",
    "fdist = nltk.FreqDist(stopwordStart_cb)\n",
    "stopwordStart_cb_freq = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "stopwordStart_cb_freq = [x[0] for x in stopwordStart_cb_freq]\n",
    "stopwordStart_cb_freq\n",
    "with open('startWithStopWords.pkl', 'wb') as f:\n",
    "    pickle.dump(stopwordStart_cb_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-clickbaits that start with stopword->  1261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'In', 'The', 'At', 'For', 'As', 'With', 'After', 'No', 'On']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordStart_ncb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    if cb == 0:\n",
    "        if title_list[0].lower() in stopWords:\n",
    "            stopwordStart_ncb.append(title_list[0])\n",
    "    \n",
    "print(\"Non-clickbaits that start with stopword-> \", len(stopwordStart_ncb))\n",
    "\n",
    "fdist = nltk.FreqDist(stopwordStart_ncb)\n",
    "stopwordStart_ncb_freq = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "stopwordStart_ncb_freq = [x[0] for x in stopwordStart_ncb_freq]\n",
    "stopwordStart_ncb_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortened Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_wrds = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = title.split()\n",
    "    if cb == 1:\n",
    "        for title in title_list:\n",
    "            t = title\n",
    "            apos_present = False\n",
    "            for ch in title:\n",
    "                if ch == \"'\":\n",
    "                    apos_present = True\n",
    "            if apos_present:\n",
    "                shortened_wrds.append(title)\n",
    "shortened_wrds\n",
    "fsdist = nltk.FreqDist(shortened_wrds)\n",
    "shortened_wrds_freq = sorted(fsdist.items(), key=itemgetter(1), reverse=True)[:29]\n",
    "shortened_wrds_freq = [x[0] for x in shortened_wrds_freq]\n",
    "shortened_wrds_freq.extend([\"That's\",\"Shouldn't\",\"Everyone's\",\"Haven't\"])\n",
    "shortened_wrds_freq\n",
    "with open('shortenedWords.pkl', 'wb') as f:\n",
    "    pickle.dump(shortened_wrds_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
