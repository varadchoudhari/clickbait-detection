{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import pickle\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many clickbait and non clickbait titles start with a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 5940\n",
      "Non-Clickbait: 306\n"
     ]
    }
   ],
   "source": [
    "count_start_digit_cb = 0\n",
    "count_start_digit_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0].isdigit():\n",
    "            count_start_digit_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0].isdigit():\n",
    "            count_start_digit_ncb += 1\n",
    "print(\"Clickbait: \"+str(count_start_digit_cb))            \n",
    "print(\"Non-Clickbait: \"+str(count_start_digit_ncb))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that mostly clickbait titles start with a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many clickbait and non clickbit titles has \"TOP + DIGIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 0\n",
      "Non-Clickbait: 0\n"
     ]
    }
   ],
   "source": [
    "count_start_top_digit_cb = 0\n",
    "count_start_top_digit_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] is \"Top\" and sp[1].isdigit():\n",
    "            count_start_top_digit_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] is \"Top\" and sp[1].isdigit():\n",
    "            count_start_top_digit_ncb += 1\n",
    "print(\"Clickbait: \"+str(count_start_top_digit_cb))            \n",
    "print(\"Non-Clickbait: \"+str(count_start_top_digit_cb))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that this data set does not contain phrases like \"Top 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the most common first word in clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_first_word_clickbait = {}\n",
    "most_common_first_word_non_clickbait = {}\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] in most_common_first_word_clickbait:\n",
    "            most_common_first_word_clickbait[sp[0]] += 1\n",
    "        else:\n",
    "            most_common_first_word_clickbait[sp[0]] = 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        if sp[0] in most_common_first_word_non_clickbait:\n",
    "            most_common_first_word_non_clickbait[sp[0]] += 1\n",
    "        else:\n",
    "            most_common_first_word_non_clickbait[sp[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_first_word_clickbait_df = pd.DataFrame()\n",
    "most_common_first_word_clickbait_df['Word'] = most_common_first_word_clickbait.keys()\n",
    "most_common_first_word_clickbait_df['Count'] = most_common_first_word_clickbait.values()\n",
    "\n",
    "most_common_first_word_non_clickbait_df = pd.DataFrame()\n",
    "most_common_first_word_non_clickbait_df['Word'] = most_common_first_word_non_clickbait.keys()\n",
    "most_common_first_word_non_clickbait_df['Count'] = most_common_first_word_non_clickbait.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>This</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Which</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "36   This   1071\n",
       "30  Which    754\n",
       "8      17    646\n",
       "4      21    618\n",
       "17    The    607"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_first_word_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>US</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>New</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>A</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>In</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "169    US    299\n",
       "39    New    235\n",
       "21   U.S.    212\n",
       "140     A    205\n",
       "49     In    176"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_first_word_non_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this we can see that most common first word in a clickbait title is \"This, Which\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check most common words in clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_clickbait = {}\n",
    "most_common_words_non_clickbait = {}\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        for word in sp:\n",
    "            if word in most_common_words_clickbait:\n",
    "                most_common_words_clickbait[word] += 1\n",
    "            else:\n",
    "                most_common_words_clickbait[word] = 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        for word in sp:\n",
    "            if word in most_common_words_non_clickbait:\n",
    "                most_common_words_non_clickbait[word] += 1\n",
    "            else:\n",
    "                most_common_words_non_clickbait[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words_clickbait_df = pd.DataFrame()\n",
    "most_common_words_clickbait_df['Word'] = most_common_words_clickbait.keys()\n",
    "most_common_words_clickbait_df['Count'] = most_common_words_clickbait.values()\n",
    "\n",
    "most_common_words_non_clickbait_df = pd.DataFrame()\n",
    "most_common_words_non_clickbait_df['Word'] = most_common_first_word_non_clickbait.keys()\n",
    "most_common_words_non_clickbait_df['Count'] = most_common_first_word_non_clickbait.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>You</td>\n",
       "      <td>4804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>To</td>\n",
       "      <td>3231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>A</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Your</td>\n",
       "      <td>2536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "96   You   4804\n",
       "13   The   4711\n",
       "38    To   3231\n",
       "53     A   2600\n",
       "59  Your   2536"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>US</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>New</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>A</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>In</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "169    US    299\n",
       "39    New    235\n",
       "21   U.S.    212\n",
       "140     A    205\n",
       "49     In    176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words_non_clickbait_df.sort_values(by=['Count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles contain functional words than non clickbait titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check average length of clickbait and non clickbait titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cb = []\n",
    "avg_ncb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        avg_cb.append(len(sp))\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        avg_ncb.append(len(sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Clickbait Title Lenght:  9.942683917744858\n",
      "Average Non-Clickbait Title Lenght:  8.19498781326167\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Clickbait Title Lenght:  \"+str(np.array(avg_cb).mean()))\n",
    "print(\"Average Non-Clickbait Title Lenght:  \"+str(np.array(avg_ncb).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles have longer length that non-clickbait titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: 217\n",
      "Nonclickbait: 1\n"
     ]
    }
   ],
   "source": [
    "the_number_cb = 0\n",
    "the_number_ncb = 0\n",
    "for index, value in crawled_data.iterrows():\n",
    "    if value[1] is 1:\n",
    "        sp = value[0].split(\" \")\n",
    "        for i in range(len(sp)-1):\n",
    "            if sp[i] == \"The\" and sp[i+1].isdigit():\n",
    "                the_number_cb += 1\n",
    "    if value[1] is 0:\n",
    "        sp = value[0].split(\" \")\n",
    "        for i in range(len(sp)-1):\n",
    "            if sp[i] == \"The\" and sp[i+1].isdigit():\n",
    "                the_number_ncb += 1\n",
    "\n",
    "print(\"Clickbait: \"+str(the_number_cb))\n",
    "print(\"Nonclickbait: \"+str(the_number_ncb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> From this we can see that mostly clickbait titles the bigram \"The NUMBER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stop words in clickbaits Vs NonClickbaits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_grt_three(crawled_data, op_file):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stopwordSeq = []\n",
    "    for index, value in crawled_data.iterrows():\n",
    "        title = value[0]\n",
    "        cb = value[1]\n",
    "        title_list = title.split(\" \")\n",
    "        title_sw_len = 0\n",
    "        for word in title_list:\n",
    "            if word.lower() in stopWords:\n",
    "    #             print(word)\n",
    "                title_sw_len += 1\n",
    "        if title_sw_len >= 3:\n",
    "            stopwordSeq.append(1)\n",
    "        else:\n",
    "            stopwordSeq.append(0)\n",
    "\n",
    "    # stopwordSeq\n",
    "    crawled_data[\"stopwords\"] = stopwordSeq\n",
    "\n",
    "    with open(op_file, 'wb') as f:\n",
    "        pickle.dump(stopwordSeq, f)\n",
    "    \n",
    "    print(\"Number of clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1]))\n",
    "\n",
    "    print(\"Number of clickbaits with zero stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1 and value[2] == 0]))\n",
    "\n",
    "    print(\"Number of non clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0]))\n",
    "\n",
    "    print(\"Number of non clickbaits with zero stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0 and value[2] == 0]))\n",
    "    \n",
    "    print(\"Generated dump in \", op_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average stopwords in clickbaits->  0.778236139758735\n",
      "Average stopwords in Non-clickbaits->  0.2276107743266046\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "avg_sw_in_cb = mean([value[2] for index, value in crawled_data.iterrows() if value[1] == 1])\n",
    "print(\"Average stopwords in clickbaits-> \", avg_sw_in_cb)\n",
    "avg_sw_in_ncb = mean([value[2] for index, value in crawled_data.iterrows() if value[1] == 0])\n",
    "print(\"Average stopwords in Non-clickbaits-> \", avg_sw_in_ncb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a Threshold. Updating the third row with number of stopwords.\n",
    "from nltk.corpus import stopwords \n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopwordSeq = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = word_tokenize(title)\n",
    "    title_sw_len = 0\n",
    "    for word in title_list:\n",
    "        if word.lower() in stopWords:\n",
    "#             print(word)\n",
    "            title_sw_len += 1\n",
    "#     if title_sw_len >= 3:\n",
    "    stopwordSeq.append(title_sw_len)\n",
    "#     else:\n",
    "#         stopwordSeq.append(0)\n",
    "    \n",
    "# stopwordSeq\n",
    "crawled_data[\"stopwords\"] = stopwordSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  15999\n",
      "Number of clickbaits with atleast 3 stopwords->  12451\n",
      "Number of non clickbaits->  16001\n",
      "Number of non clickbaits with atleast 3 stopwords->  3642\n"
     ]
    }
   ],
   "source": [
    "# Adding a Threshold\n",
    "\n",
    "print(\"Number of clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1]))\n",
    "\n",
    "print(\"Number of clickbaits with atleast 3 stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 1 and value[2] >= 3]))\n",
    "\n",
    "print(\"Number of non clickbaits-> \", len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0]))\n",
    "\n",
    "print(\"Number of non clickbaits with atleast 3 stopwords-> \",len([value[0] for index, value in crawled_data.iterrows() if value[1] == 0 and value[2] >= 3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 25 Common Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_common_bigrams(crawled_data, op_file):\n",
    "    cb_bigrams = []\n",
    "    for index, value in crawled_data.iterrows():\n",
    "        if value[1] == 1:\n",
    "            title_list = value[0].split(\" \")\n",
    "            cb_bigrams.extend(list(nltk.bigrams(title_list)))\n",
    "\n",
    "    fdist = nltk.FreqDist(cb_bigrams)\n",
    "    common_bigrams = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:25]\n",
    "    # print(common_bigrams)\n",
    "    common_bigrams = [' '.join(ct[0]) for ct in common_bigrams]\n",
    "    print(common_bigrams)\n",
    "    with open(op_file, 'wb') as f:\n",
    "        pickle.dump(common_bigrams, f)\n",
    "        print(\"Generated 25 Common Bigrams dump in -> \", op_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_common_trigrams(crawled_data, op_file):\n",
    "    cb_trigrams = []\n",
    "    for index, value in crawled_data.iterrows():\n",
    "        if value[1] == 1:\n",
    "            title_list = value[0].split(\" \")\n",
    "            c = 0\n",
    "            while c < len(title_list) - 2:\n",
    "                cb_trigrams.append((title_list[c], title_list[c+1], title_list[c+2]))\n",
    "                c += 1\n",
    "    #         cb_trigrams.extend(list(nltk.trigrams(title_list)))\n",
    "\n",
    "    fdisttri = nltk.FreqDist(cb_trigrams)\n",
    "    common_trigrams = sorted(fdisttri.items(), key=itemgetter(1), reverse=True)[:50]\n",
    "    common_trigrams = [' '.join(ct[0]) for ct in common_trigrams]\n",
    "    print(common_trigrams)\n",
    "    with open(op_file, 'wb') as f:\n",
    "        pickle.dump(common_trigrams, f)\n",
    "        print(\"Generated 50 common Trigrams in -> \", op_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 stopwords with which clickbaits start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_top_stop_words(crawled_data, op_file):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stopwordStart_cb = []\n",
    "    for index, value in crawled_data.iterrows():\n",
    "        title = value[0]\n",
    "        cb = value[1]\n",
    "        title_list = title.split(\" \")\n",
    "        if cb == 1:\n",
    "            if title_list[0].lower() in stopWords:\n",
    "                stopwordStart_cb.append(title_list[0])\n",
    "\n",
    "    print(\"clickbaits that start with stopword-> \", len(stopwordStart_cb))\n",
    "    stopwordStart_cb\n",
    "    fdist = nltk.FreqDist(stopwordStart_cb)\n",
    "    stopwordStart_cb_freq = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "    stopwordStart_cb_freq = [x[0] for x in stopwordStart_cb_freq]\n",
    "    stopwordStart_cb_freq\n",
    "    with open(op_file, 'wb') as f:\n",
    "        pickle.dump(stopwordStart_cb_freq, f)\n",
    "        print(\"Generated top 10 starting stopwords in -> \", op_file)\n",
    "    print(stopwordStart_cb_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-clickbaits that start with stopword->  1239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'In', 'The', 'At', 'For', 'As', 'With', 'After', 'On', 'No']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "stopwordStart_ncb = []\n",
    "for index, value in crawled_data.iterrows():\n",
    "    title = value[0]\n",
    "    cb = value[1]\n",
    "    title_list = title.split(\" \")\n",
    "    if cb == 0:\n",
    "        if title_list[0].lower() in stopWords:\n",
    "            stopwordStart_ncb.append(title_list[0])\n",
    "    \n",
    "print(\"Non-clickbaits that start with stopword-> \", len(stopwordStart_ncb))\n",
    "\n",
    "fdist = nltk.FreqDist(stopwordStart_ncb)\n",
    "stopwordStart_ncb_freq = sorted(fdist.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "stopwordStart_ncb_freq = [x[0] for x in stopwordStart_ncb_freq]\n",
    "stopwordStart_ncb_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 30 Shortened Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_top_shortened_words(crawled_data, op_file):\n",
    "    shortened_wrds = []\n",
    "    for index, value in crawled_data.iterrows():\n",
    "        title = value[0]\n",
    "        cb = value[1]\n",
    "        title_list = title.split()\n",
    "        if cb == 1:\n",
    "            for title in title_list:\n",
    "                t = title\n",
    "                apos_present = False\n",
    "                for idx, ch in enumerate(title):\n",
    "                    if ch == \"'\" and idx < len(title)-1 and idx > 0:\n",
    "                        apos_present = True\n",
    "                if apos_present:\n",
    "                    shortened_wrds.append(title)\n",
    "    shortened_wrds\n",
    "    fsdist = nltk.FreqDist(shortened_wrds)\n",
    "    shortened_wrds_freq = sorted(fsdist.items(), key=itemgetter(1), reverse=True)[:29]\n",
    "    shortened_wrds_freq = [x[0] for x in shortened_wrds_freq]\n",
    "    shortened_wrds_freq.extend([\"That's\",\"Shouldn't\",\"Everyone's\",\"Haven't\"])\n",
    "    print(shortened_wrds_freq)\n",
    "    with open(op_file, 'wb') as f:\n",
    "        pickle.dump(shortened_wrds_freq, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation for different dattasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  15999\n",
      "Number of clickbaits with zero stopwords->  3756\n",
      "Number of non clickbaits->  16001\n",
      "Number of non clickbaits with zero stopwords->  12423\n",
      "Generated dump in  dataset/iit/greaterThanThreeSW.pkl\n",
      "['Are You', 'Based On', 'Do You', 'On Your', 'That Will', 'Can You', 'Make You', 'Of The', 'That Are', 'We Know', 'Will Make', 'The Most', 'The Best', 'People Who', 'Need To', 'Your Zodiac', 'In The', 'In 2015', 'This Is', 'You Need', 'Your Favorite', 'Is The', 'Should You', 'Zodiac Sign', 'For The']\n",
      "Generated 25 Common Bigrams dump in ->  dataset/iit/commonBigrams.pkl\n",
      "['Based On Your', 'Will Make You', 'That Will Make', 'On Your Zodiac', 'Your Zodiac Sign', 'How Well Do', 'Well Do You', 'You Need To', 'This Is What', 'Are You More', 'You Based On', 'Are You Based', 'We Know Your', 'Make You Laugh', 'Do You Remember', 'Do You Know', 'Of The Most', 'Can We Guess', 'Can You Guess', 'For People Who', 'Character Are You', 'For The First', 'On Your Favorite', 'The First Time', 'Too Real For', 'Can You Identify', 'Do You Actually', 'Way Too Real', 'Of All Time', '\"Game Of Thrones\"', 'We Need To', 'To Know About', 'To Talk About', 'Are Way Too', \"What It's Like\", 'And It Was', 'That Are Way', \"Here's How To\", 'We Know Which', 'Need To Know', 'You Remember The', 'Know Your Favorite', \"That'll Make You\", 'We Guess Your', 'Should Be Your', 'You Know The', 'Need To Talk', 'We Know What', 'What Happens When', 'You Guess The']\n",
      "Generated 50 common Trigrams in ->  dataset/iit/commonTrigrams.pkl\n",
      "clickbaits that start with stopword->  6516\n",
      "Generated top 10 starting stopwords in ->  dataset/iit/startWithStopWords.pkl\n",
      "['This', 'Which', 'The', 'How', 'We', 'Can', 'What', 'Are', 'These', 'A']\n",
      "[\"Here's\", \"You're\", \"It's\", \"You'll\", \"What's\", \"That'll\", \"You've\", \"Don't\", \"Didn't\", \"Can't\", \"Won't\", \"Valentine's\", \"Who's\", \"Aren't\", \"Year's\", \"They're\", \"I'm\", \"There's\", \"Adele's\", \"We're\", \"Doesn't\", \"Let's\", \"You'd\", \"She's\", \"Isn't\", \"Couldn't\", \"We'll\", \"Swift's\", \"He's\", \"That's\", \"Shouldn't\", \"Everyone's\", \"Haven't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crawled_data = pd.read_csv(\"dataset/iit/dataframe/dataset.csv\")\n",
    "\n",
    "gen_grt_three(crawled_data, \"dataset/iit/greaterThanThreeSW.pkl\")\n",
    "gen_common_bigrams(crawled_data, 'dataset/iit/commonBigrams.pkl')\n",
    "gen_common_trigrams(crawled_data, 'dataset/iit/commonTrigrams.pkl')\n",
    "gen_top_stop_words(crawled_data, 'dataset/iit/startWithStopWords.pkl')\n",
    "gen_top_shortened_words(crawled_data, 'dataset/iit/shortenedWords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  16000\n",
      "Number of clickbaits with zero stopwords->  1252\n",
      "Number of non clickbaits->  16000\n",
      "Number of non clickbaits with zero stopwords->  12541\n",
      "Generated dump in  dataset/original/dataframe/greaterThanThreeSW.pkl\n",
      "['Of The', 'In The', 'Make You', 'Is The', 'The World', 'You Can', 'Here Are', 'That Are', 'Will Make', 'In A', 'This Is', 'If You', 'The Most', '& It’s', 'Need To', 'That Will', 'For The', 'You Should', 'In India', 'To Be', 'On The', 'Of A', 'To The', 'Here’s Why', 'That Prove']\n",
      "Generated 25 Common Bigrams dump in ->  dataset/original/dataframe/commonBigrams.pkl\n",
      "['Will Make You', 'You Need To', 'This Is What', 'That’ll Make You', 'Around The World', 'In The World', 'You Want To', 'Of The Most', 'That Will Make', 'Here Are The', 'Perfectly Sum Up', 'All Set To', 'Sum Up The', 'All Of Us', 'Game Of Thrones', 'The Trailer Of', 'Is Out &', 'This Is How', '& We Can’t', 'Make You Want', 'This Video Of', 'From Around The', 'In Love With', 'To Know About', 'Shah Rukh Khan', 'Why You Should', 'Did You Know', 'Will Leave You', 'Need To Know', 'Here Are 10', '& It Looks', 'If You Are', 'The World That', 'Will Give You', 'The Story Of', 'Would Look Like', 'You Won’t Believe', 'Can’t Get Over', 'The Rest Of', 'One Of The', 'Should Be On', 'The Life Of', 'Here Are Some', 'Opens Up About', 'Is All Set', 'Make You Go', 'Relate To If', 'The Results Are', 'Video Of A', 'Perfectly Capture The']\n",
      "Generated 50 common Trigrams in ->  dataset/original/dataframe/commonTrigrams.pkl\n",
      "clickbaits that start with stopword->  5596\n",
      "Generated top 10 starting stopwords in ->  dataset/original/dataframe/startWithStopWords.pkl\n",
      "['This', 'These', 'The', 'If', 'A', 'We', 'From', 'After', 'I', 'Here']\n",
      "[\"It's\", \"Doesn't\", \"Isn't\", \"Here's\", \"Can't\", \"Sehgal's\", \"One's\", \"Women's\", \"We'll\", \"That's\", \"Shouldn't\", \"Everyone's\", \"Haven't\"]\n"
     ]
    }
   ],
   "source": [
    "crawled_data = pd.read_csv(\"dataset/original/dataframe/dataset.csv\")\n",
    "\n",
    "gen_grt_three(crawled_data, \"dataset/original/dataframe/greaterThanThreeSW.pkl\")\n",
    "gen_common_bigrams(crawled_data, 'dataset/original/dataframe/commonBigrams.pkl')\n",
    "gen_common_trigrams(crawled_data, 'dataset/original/dataframe/commonTrigrams.pkl')\n",
    "gen_top_stop_words(crawled_data, 'dataset/original/dataframe/startWithStopWords.pkl')\n",
    "gen_top_shortened_words(crawled_data, 'dataset/original/dataframe/shortenedWords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clickbaits->  4716\n",
      "Number of clickbaits with zero stopwords->  1494\n",
      "Number of non clickbaits->  4760\n",
      "Number of non clickbaits with zero stopwords->  1830\n",
      "Generated dump in  dataset/clickbait_challenge/greaterThanThreeSW.pkl\n",
      "['of the', 'in the', 'need to', 'the most', 'you need', 'to know', 'are the', 'This is', 'the best', 'the world', 'How to', 'from the', \"Here's what\", 'to be', 'on the', 'for the', 'at the', 'Donald Trump', 'to the', 'that will', 'in a', 'know about', 'for a', 'about the', 'to get']\n",
      "Generated 25 Common Bigrams dump in ->  dataset/clickbait_challenge/commonBigrams.pkl\n",
      "['you need to', 'need to know', 'These are the', 'to know about', 'from the day', 'what you need', 'stories from the', \"Here's what you\", 'in the world', 'of the most', 'entertainment stories from', 'will make you', 'news stories from', 'the end of', 'know about the', 'This is what', 'Here are the', 'that will make', 'Top news stories', 'around the world', 'the day that', 'day that was', 'This is the', 'end of the', 'This is how', 'Everything you need', \"Briefing: Here's what\", 'one of the', \"the world's most\", 'at the end', 'to know to', 'know to start', 'to start your', 'are the best', 'things you need', 'This is why', 'know at the', 'of the day', 'Top entertainment stories', 'start your day', 'of the best', 'to know before', 'to help you', \"Evening Briefing: Here's\", 'to know at', 'of all time', '10 things you', 'know before the', 'before the opening', 'the opening bell']\n",
      "Generated 50 common Trigrams in ->  dataset/clickbait_challenge/commonTrigrams.pkl\n",
      "clickbaits that start with stopword->  1849\n",
      "Generated top 10 starting stopwords in ->  dataset/clickbait_challenge/startWithStopWords.pkl\n",
      "['The', 'This', 'How', 'Why', 'A', 'These', 'What', 'Here', 'Is', 'Can']\n",
      "[\"Here's\", \"Trump's\", \"it's\", \"world's\", \"don't\", \"It's\", \"can't\", \"you're\", \"won't\", \"didn't\", \"Don't\", \"What's\", \"here's\", \"There's\", \"America's\", \"year's\", \"you've\", \"That's\", \"Obama's\", \"isn't\", \"man's\", \"We're\", \"they're\", \"you'll\", \"Year's\", \"we're\", \"week's\", \"he's\", \"wasn't\", \"That's\", \"Shouldn't\", \"Everyone's\", \"Haven't\"]\n"
     ]
    }
   ],
   "source": [
    "crawled_data = pd.read_csv(\"dataset/clickbait_challenge/dataframe/dataset.csv\")\n",
    "\n",
    "gen_grt_three(crawled_data, \"dataset/clickbait_challenge/greaterThanThreeSW.pkl\")\n",
    "gen_common_bigrams(crawled_data, 'dataset/clickbait_challenge/commonBigrams.pkl')\n",
    "gen_common_trigrams(crawled_data, 'dataset/clickbait_challenge/commonTrigrams.pkl')\n",
    "gen_top_stop_words(crawled_data, 'dataset/clickbait_challenge/startWithStopWords.pkl')\n",
    "gen_top_shortened_words(crawled_data, 'dataset/clickbait_challenge/shortenedWords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
